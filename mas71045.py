# -*- coding: utf-8 -*-
"""MAS71045.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_PT7ltqVh2yYXH_stpM_TfaWK9DaUps

# 1. Load and inspect the data

We will use the Fashion MNIST dataset (built into Keras/TF), which are all small images of different clothing items. They can be found in `keras.datasets.fashion_mnist`

1. Load the dataset and display the size of the various parts
"""

# for numerical analysis
import numpy as np
# to store and process in a dataframe
import pandas as pd

# for ploting graphs
import matplotlib.pyplot as plt
# advancec ploting
import seaborn as sns

# image processing
import matplotlib.image as mpimg

# train test split
from sklearn.model_selection import train_test_split
# model performance metrics
from sklearn.metrics import confusion_matrix, classification_report

# utility functions
from tensorflow.keras.utils import to_categorical
# sequential model
from tensorflow.keras.models import Sequential
# layers
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout

"""train & test cloths"""

import matplotlib.pyplot as plt

# Define class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Function to plot images
def plot_images(images, labels, nrows, ncols):
    plt.figure(figsize=(10, 10))
    for i in range(nrows * ncols):
        plt.subplot(nrows, ncols, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[np.argmax(labels[i])])
    plt.show()

# Plot some random images from the training set
plot_images(X_train, y_train, 5, 5)

# Plot some random images from the test set
plot_images(X_test, y_test, 5, 5)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist

# Load the Fashion MNIST dataset
(X_train, y_train), (_, _) = fashion_mnist.load_data()

# Display one example image of each class
num_images = 10  # Total number of images to display
num_rows = 2
num_cols = num_images // num_rows

# Map class labels to human-readable names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 6))

for i in range(num_images):
    row = i // num_cols
    col = i % num_cols
    axs[row, col].imshow(X_train[i], cmap='gray')
    axs[row, col].set_title(class_names[y_train[i]])
    axs[row, col].axis('off')

plt.tight_layout()
plt.show()

"""2. Display, graphically, one example image of each class"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist

# Load the Fashion MNIST dataset
(X_train, y_train), (_, _) = fashion_mnist.load_data()

# Define the class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Display one example image of each class
fig, axs = plt.subplots(1, len(class_names), figsize=(20, 3))

for i, class_name in enumerate(class_names):
    class_indices = np.where(y_train == i)[0]
    example_index = class_indices[0]
    axs[i].imshow(X_train[example_index], cmap='gray')
    axs[i].set_title(class_name)
    axs[i].axis('off')

plt.show()

"""3. Display summary stats/info for class labels"""

import numpy as np
from collections import Counter
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt

# Load the Fashion MNIST dataset
(_, y_train), (_, _) = fashion_mnist.load_data()

# Define the class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Count occurrences of each class label
class_counts = Counter(y_train)

# Display summary stats/info for class labels
print("Class Counts:")
for class_index, count in class_counts.items():
    print(f"{class_names[class_index]}: {count} examples")

# Visualize class distribution
plt.figure(figsize=(10, 6))
plt.bar(class_names, class_counts.values())
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Number of Examples')
plt.xticks(rotation=45)
plt.show()

"""4. Split the non-test labels and images into training and validation datasets according to a 80/20 split"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import fashion_mnist

# Load the Fashion MNIST dataset
(X_train_all, y_train_all), (_, _) = fashion_mnist.load_data()

# Split the non-test labels and images into training and validation datasets (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)

# Display the sizes of the training and validation datasets
print("Training data shape:", X_train.shape)
print("Training labels shape:", y_train.shape)
print("Validation data shape:", X_val.shape)
print("Validation labels shape:", y_val.shape)

"""5. Do appropriate pre-processing of the images and/or labels"""

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train_all, y_train_all), (_, _) = fashion_mnist.load_data()

# Normalize the pixel values to be between 0 and 1
X_train_all = X_train_all.astype('float32') / 255.0

# Reshape the images to have a single channel (grayscale)
X_train_all = np.expand_dims(X_train_all, axis=-1)

# Convert the labels to one-hot encoding
y_train_all = to_categorical(y_train_all, num_classes=10)

# Split the data into training and validation sets (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)

# Display the shapes of the training and validation sets
print("Training data shape:", X_train.shape)
print("Training labels shape:", y_train.shape)
print("Validation data shape:", X_val.shape)
print("Validation labels shape:", y_val.shape)

"""# 2. Build a network

Write a *function* that creates a Keras model of a densely connected neural network and returns it, in a state where it is ready to have `fit()` run on it.
 - The API should take in:
     1. values for the number of neurons in hidden layers 1, 2 and 3 (as a list or array - where the length of the list/array indicates fewer hidden layers)
     2. a string for the loss function
     3. the learning rate
 - In addition:
     4. set the optimizer to be Adam
     5. set the activation functions in the hidden layers to be ReLU
     6. include the additional metric of 'accuracy'

"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model

def build_cnn_model(input_shape, num_classes, loss_function='categorical_crossentropy', learning_rate=0.001):
    model = Sequential()

    # Convolutional layers
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Fully connected layers
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss=loss_function,
                  metrics=['accuracy'])

    return model

# Assuming input shape and number of classes
input_shape = (28, 28, 1)  # Example for MNIST dataset
num_classes = 10  # Example for MNIST dataset

# Build the CNN model
cnn_model = build_cnn_model(input_shape, num_classes)

# Visualize the model architecture
plot_model(cnn_model, to_file='cnn_model.png', show_shapes=True)

!pip install idx2numpy

import idx2numpy
import matplotlib.pyplot as plt
import numpy as np

# Load the images and labels from the IDX format files
images_path = "/content/t10k-images-idx3-ubyte"
labels_path = "/content/t10k-labels-idx1-ubyte"

X_test = idx2numpy.convert_from_file(images_path)
y_test = idx2numpy.convert_from_file(labels_path)

# Define class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Convert grayscale images to RGB format
X_test_color = np.stack((X_test,) * 3, axis=-1)

# Plot some random clothing images
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_test_color[i])
    plt.xlabel(class_names[y_test[i]])
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def build_dense_nn(hidden_neurons, loss_function, learning_rate):
    # Initialize the model
    model = Sequential()

    # Add the input layer
    model.add(Dense(hidden_neurons[0], activation='relu', input_shape=(784,)))  # Assuming input shape is 784

    # Add hidden layers
    for neurons in hidden_neurons[1:]:
        model.add(Dense(neurons, activation='relu'))

    # Add the output layer
    model.add(Dense(10, activation='softmax'))  # Assuming output size is 10 for classification

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss=loss_function,
                  metrics=['accuracy'])

    return model

    # Build the model
model = build_dense_nn(hidden_neurons=[128, 64, 32], loss_function='categorical_crossentropy', learning_rate=0.001)

# Print the model summary
print(model.summary())

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize the pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the input data to include the channel dimension (for grayscale images)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Convert the labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Plot training history
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""# 3. Train the network

1. Call the function written in the previous section to generate a model with the following arguments: 2 hidden layers only, first with 128 nodes and the second with 64 nodes; loss function of categorical cross-entropy (you should determine which type is suitable for your data); and a learning rate of 0.001.
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the input data to include the channel dimension (for grayscale images)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the model
model = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

import struct
import numpy as np

def load_mnist(path, kind='train'):
    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte')
    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte')

    with open(labels_path, 'rb') as labelpath:
        magic, num = struct.unpack('>II', labelpath.read(8))
        labels = np.fromfile(labelpath, dtype=np.uint8)

    with open(images_path, 'rb') as imgpath:
        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))
        images = np.fromfile(imgpath, dtype=np.uint8)
        images = images.reshape(-1, rows, cols)
        print("Images shape:", images.shape)
    return images, labels

"""2. Print a summary of the model that is generated."""

# Print the summary of the model
print("Model Summary:")
print(model.summary())

"""3. Train the model using a batch size of 2000 and 5 epochs, saving the training history."""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the input data to include the channel dimension (for grayscale images)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the model
model = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model with batch size of 2000 and 5 epochs
history = model.fit(X_train, y_train, epochs=5, batch_size=2000, validation_split=0.2)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the input data to include the channel dimension (for grayscale images)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train))

# Convert labels to one-hot encoding
y_train_one_hot = to_categorical(y_train, num_classes=num_classes)
y_test_one_hot = to_categorical(y_test, num_classes=num_classes)

# Define the model
model = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')  # Output layer with dynamic number of units
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model with batch size of 2000 and 5 epochs
history = model.fit(X_train, y_train_one_hot, validation_split=0.2, batch_size=2000, epochs=5, verbose=1)

print("Shape of y_train_one_hot:", y_train_one_hot.shape)
print("Shape of y_test_one_hot:", y_test_one_hot.shape)

"""4. Display the learning curves."""

import matplotlib.pyplot as plt

# Plot the training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot the training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""5. Answer the following questions on fitting (give reasons in each case):
 - Does this show overfitting?
 - Does this show underfitting?
 - Has the model converged?
 - Is it stable or unstable?

***Your answers here...***

# 4. Network architecture comparison

Build and train two alternative networks.

1.1. Build and train a network using a sigmoid activation function in the hidden layers, plotting the learning curves.
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.utils import to_categorical

# Load the Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Reshape the input data to include the channel dimension (for grayscale images)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the model with sigmoid activation function in hidden layers
model_sigmoid = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='sigmoid'),
    Dense(64, activation='sigmoid'),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model_sigmoid.compile(optimizer=Adam(learning_rate=0.001),
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

# Train the model
history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Plot the learning curves
plt.plot(history_sigmoid.history['accuracy'], label='Training Accuracy')
plt.plot(history_sigmoid.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy - Sigmoid Activation')
plt.legend()
plt.show()

"""1.2. Build and train a network using a LeakyReLU activation function in the hidden layers, plotting the learning curves."""

from tensorflow.keras.layers import LeakyReLU

# Define the model with LeakyReLU activation function in hidden layers
model_leaky_relu = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation=LeakyReLU(alpha=0.1)),
    Dense(64, activation=LeakyReLU(alpha=0.1)),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model_leaky_relu.compile(optimizer=Adam(learning_rate=0.001),
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])

# Train the model
history_leaky_relu = model_leaky_relu.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Plot the learning curves
plt.plot(history_leaky_relu.history['accuracy'], label='Training Accuracy')
plt.plot(history_leaky_relu.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy - LeakyReLU Activation')
plt.legend()
plt.show()

"""2. Compare the performance appropriately to select the best activation function from these two and ReLU above (state your reasons).

"""

# Plot the learning curves for ReLU, LeakyReLU, and Sigmoid
plt.figure(figsize=(10, 6))

# Plot ReLU
plt.plot(history.history['val_accuracy'], label='ReLU')

# Plot LeakyReLU
plt.plot(history_leaky_relu.history['val_accuracy'], label='LeakyReLU')

# Plot Sigmoid
plt.plot(history_sigmoid.history['val_accuracy'], label='Sigmoid')

plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy Comparison')
plt.legend()
plt.grid(True)
plt.show()

# Compare final validation accuracies
relu_val_accuracy = history.history['val_accuracy'][-1]
leaky_relu_val_accuracy = history_leaky_relu.history['val_accuracy'][-1]
sigmoid_val_accuracy = history_sigmoid.history['val_accuracy'][-1]

print("Final Validation Accuracies:")
print("ReLU:", relu_val_accuracy)
print("LeakyReLU:", leaky_relu_val_accuracy)
print("Sigmoid:", sigmoid_val_accuracy)

# Select the best activation function based on validation accuracy
best_activation = max([('ReLU', relu_val_accuracy), ('LeakyReLU', leaky_relu_val_accuracy), ('Sigmoid', sigmoid_val_accuracy)],
                      key=lambda x: x[1])

print("Best Activation Function:", best_activation[0])

""" 3. Repeat the model building and training for 4.1.1 and 4.1.2 above and compare the new results with the previous ones. How different are the results?  And how important is this difference?"""

# Define the model with ReLU activation function in hidden layers
model_relu_new = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model_relu_new.compile(optimizer=Adam(learning_rate=0.001),
                       loss='categorical_crossentropy',
                       metrics=['accuracy'])

# Train the model
history_relu_new = model_relu_new.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Define the model with LeakyReLU activation function in hidden layers
model_leaky_relu_new = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation=LeakyReLU(alpha=0.1)),
    Dense(64, activation=LeakyReLU(alpha=0.1)),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model_leaky_relu_new.compile(optimizer=Adam(learning_rate=0.001),
                              loss='categorical_crossentropy',
                              metrics=['accuracy'])

# Train the model
history_leaky_relu_new = model_leaky_relu_new.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Compare the new results with the previous ones
relu_val_accuracy_new = history_relu_new.history['val_accuracy'][-1]
leaky_relu_val_accuracy_new = history_leaky_relu_new.history['val_accuracy'][-1]

print("New Validation Accuracies:")
print("ReLU (New):", relu_val_accuracy_new)
print("LeakyReLU (New):", leaky_relu_val_accuracy_new)

# Compare with previous results
print("\nComparison with Previous Results:")
print("ReLU (Previous):", relu_val_accuracy)
print("LeakyReLU (Previous):", leaky_relu_val_accuracy)

"""# 5. Hyper-parameter optimisation

1. Implement early-stopping using an appropriate callback, included in the call to fit() - see example in Module 4.
"""

from tensorflow.keras.callbacks import EarlyStopping

# Define the model
model = Sequential([
    Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model with early stopping
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])

"""2. Run the network with a range of learning rates (0.01, 0.1, 10 and 100 _times_ the base value used above) and compare learning curves to choose the best rate."""

learning_rates = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]

# Define lists to store training history for each learning rate
histories = []

# Iterate over each learning rate
for lr in learning_rates:
    # Define the model
    model = Sequential([
        Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
    ])

    # Compile the model with the current learning rate
    model.compile(optimizer=Adam(learning_rate=lr),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model
    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

    # Append the training history to the list
    histories.append(history)

# Plot the learning curves for each learning rate
plt.figure(figsize=(10, 6))
for i, history in enumerate(histories):
    plt.plot(history.history['val_accuracy'], label=f'LR={learning_rates[i]}')

plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy Comparison for Different Learning Rates')
plt.legend(title='Learning Rate')
plt.grid(True)
plt.show()

"""3. After having chosen the best learning rate, do a simple parameter sweep for the number of nodes in the first hidden layer (using 16, 32, 64, 128, 256, 512, 1024) and display both individual learning curves as well as a summary plot of the results showing performance vs number of nodes.
     - Note that it you want you can make the output more concise by suppressing text output with the _verbose=0_ option to fit()
"""

# Define a list of number of nodes in the first hidden layer
num_nodes_list = [16, 32, 64, 128, 256, 512, 1024]

# Define a dictionary to store training history for each number of nodes
history_dict = {}

# Best learning rate chosen previously
best_learning_rate = 0.01  # Placeholder value, replace it with the actual best learning rate

# Iterate over each number of nodes
for num_nodes in num_nodes_list:
    # Define the model with the current number of nodes
    model = Sequential([
        Flatten(input_shape=(28, 28, 1)),  # Flatten the input data
        Dense(num_nodes, activation='relu'),
        Dense(64, activation='relu'),
        Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes
    ])

    # Compile the model with the best learning rate chosen previously
    model.compile(optimizer=Adam(learning_rate=best_learning_rate),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model with verbose=0 to suppress text output
    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=0)

    # Store the training history for the current number of nodes
    history_dict[num_nodes] = history

# Plot individual learning curves
plt.figure(figsize=(12, 8))
for num_nodes, history in history_dict.items():
    plt.plot(history.history['val_accuracy'], label=f'Nodes={num_nodes}')

plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy for Different Number of Nodes in First Hidden Layer')
plt.legend(title='Number of Nodes')
plt.grid(True)
plt.show()

# Plot summary of performance vs number of nodes
plt.figure(figsize=(10, 6))
plt.plot(num_nodes_list, [history.history['val_accuracy'][-1] for history in history_dict.values()], marker='o')
plt.xlabel('Number of Nodes in First Hidden Layer')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs Number of Nodes in First Hidden Layer')
plt.grid(True)
plt.show()

"""4. From all of the above results, choose the best hyper-parameters and clearly state your reasons. Include a comment on how reliable and repeatable you think this choice is.

***Your choice and explanations here***

# 6. Dropout

1. Using the best hyper-parameters from above, implement this model but include a dropout layer (with rate 0.4) between the last dense layer and the output layer.
"""

from tensorflow.keras.layers import Dropout

def build_dense_neural_network_with_dropout(hidden_neurons, loss_function, learning_rate):
    model = Sequential()
    model.add(Dense(hidden_neurons[0], activation='relu', input_shape=(28*28,)))
    for neurons in hidden_neurons[1:]:
        model.add(Dense(neurons, activation='relu'))
    model.add(Dropout(0.4))  # Add dropout layer
    model.add(Dense(10, activation='softmax'))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss_function, metrics=['accuracy'])
    return model

# Create the model with dropout
model_with_dropout = build_dense_neural_network_with_dropout(hidden_neurons=[128, 64],
                                                             loss_function='categorical_crossentropy',
                                                             learning_rate=0.001)
# Print the summary of the model with dropout
print("Model Summary with Dropout:")
print(model_with_dropout.summary())

"""2. Train this and compare the result with the one from the equivalent network without dropout and decide whether the network performs better with dropout or not. Show learning curves and a brief summary of the results and your decision.

1. Train the network without dropout:
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Define the model without dropout
model_without_dropout = Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model_without_dropout.compile(optimizer=Adam(learning_rate=0.01),
                              loss='categorical_crossentropy',
                              metrics=['accuracy'])

# Train the model without dropout
history_without_dropout = model_without_dropout.fit(X_train, y_train,
                                                    epochs=10,
                                                    batch_size=128,
                                                    validation_split=0.2,
                                                    verbose=1)

# Evaluate the model without dropout on the test set
test_loss_without_dropout, test_accuracy_without_dropout = model_without_dropout.evaluate(X_test, y_test)

# Plot learning curves for the model without dropout
plt.plot(history_without_dropout.history['accuracy'], label='Training Accuracy')
plt.plot(history_without_dropout.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Learning Curves without Dropout')
plt.legend()
plt.grid(True)
plt.show()

# Summary of the results
print("Results without Dropout:")
print("Test Loss:", test_loss_without_dropout)
print("Test Accuracy:", test_accuracy_without_dropout)

"""2. Train the network with dropout"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Define the model with dropout
model_with_dropout = Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(128, activation='relu'),
    Dropout(0.2),  # Dropout layer with a dropout rate of 0.2
    Dense(64, activation='relu'),
    Dropout(0.2),  # Dropout layer with a dropout rate of 0.2
    Dense(10, activation='softmax')
])

# Compile the model
model_with_dropout.compile(optimizer=Adam(learning_rate=0.01),
                           loss='categorical_crossentropy',
                           metrics=['accuracy'])

# Train the model with dropout
history_with_dropout = model_with_dropout.fit(X_train, y_train,
                                              epochs=10,
                                              batch_size=128,
                                              validation_split=0.2,
                                              verbose=1)

# Evaluate the model with dropout on the test set
test_loss_with_dropout, test_accuracy_with_dropout = model_with_dropout.evaluate(X_test, y_test)

# Summary of the results
print("Results with Dropout:")
print("Test Loss:", test_loss_with_dropout)
print("Test Accuracy:", test_accuracy_with_dropout)

"""Plot the learning curves"""

# Plot learning curves for the model without dropout
plt.figure(figsize=(12, 6))
plt.plot(history_without_dropout.history['accuracy'], label='Training Accuracy (without Dropout)')
plt.plot(history_without_dropout.history['val_accuracy'], label='Validation Accuracy (without Dropout)')

# Plot learning curves for the model with dropout
plt.plot(history_with_dropout.history['accuracy'], label='Training Accuracy (with Dropout)')
plt.plot(history_with_dropout.history['val_accuracy'], label='Validation Accuracy (with Dropout)')

plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Learning Curves with and without Dropout')
plt.legend()
plt.grid(True)
plt.show()

"""***Your summary here***

# 7. Final evaluation

1. Evaluate final performance using the best network (chosen from all the options you have explored above)
"""

from tensorflow.keras.utils import to_categorical

# Load the images and labels from the IDX format files
images_path = "/content/t10k-images-idx3-ubyte"
labels_path = "/content/t10k-labels-idx1-ubyte"

X_test = idx2numpy.convert_from_file(images_path)
y_test = idx2numpy.convert_from_file(labels_path)

# Reshape the input data to include the channel dimension (for grayscale images)
X_test = np.expand_dims(X_test, axis=-1)

# Normalize pixel values to the range [0, 1]
X_test = X_test.astype('float32') / 255.0

# One-hot encode the target labels
y_test = to_categorical(y_test)

# Split the data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=42)

# Define function to create a neural network model
def create_model(neurons_list, loss_function, learning_rate):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28, 1)))  # Flatten the input data
    for i, neurons in enumerate(neurons_list, start=1):
        model.add(Dense(neurons, activation='relu', name=f'hidden_layer_{i}'))  # Add hidden layers with ReLU activation
    model.add(Dense(10, activation='softmax', name='output_layer'))  # Output layer with softmax activation
    # Compile the model with Adam optimizer and specified loss function and learning rate
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss_function, metrics=['accuracy'])
    return model

# Define the configurations to test
configurations = [
    ([64, 64], 'categorical_crossentropy', 0.001),
    ([128, 64, 32], 'categorical_crossentropy', 0.001),
    ([256, 128, 64, 32], 'categorical_crossentropy', 0.001)
]

# Train models with different configurations
for config in configurations:
    model = create_model(*config)
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)
    val_accuracy = history.history['val_accuracy'][-1]
    print(f"Validation Accuracy for configuration {config}: {val_accuracy}")

# Plot learning curves for each configuration
plt.figure(figsize=(12, 8))
for i, config in enumerate(configurations, start=1):
    plt.subplot(2, 2, i)
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.plot(history.history['val_accuracy'], label='val_accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title(f'Configuration {i}')
    plt.legend()
plt.tight_layout()
plt.show()

"""2. Calculate the confusion matrix from these results and show the matrix graphically"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Get the predicted probabilities for each class
y_pred_probs = model.predict(X_test)

# Get the predicted labels by selecting the class with the highest probability
y_pred = np.argmax(y_pred_probs, axis=1)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

# Plot the confusion matrix graphically
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""3. Identify which class is most often incorrectly classified, explaining your reasoning

***Your choice and explanation here***

4. If the correct class is number 6 (shirt), which is the most likely error that your network will make (explain your reasoning).

***Your result and explanation here***
"""